{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8LdRW2U-7zqO"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "apt-get install openjdk-17-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "97EBJRBF74QO"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "wget -q https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz \n",
        "tar xf spark-3.3.0-bin-hadoop3.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "thtwIov_7kG9",
        "outputId": "53b00636-29c5-4970-e033-e6109d12e20c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "siMXRQzc8O8u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['JAVA_HOME']='/usr/lib/jvm/java-1.17.0-openjdk-amd64'\n",
        "os.environ['SPARK_HOME']='/content/spark-3.3.0-bin-hadoop3'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nkW9vzJO8eNw"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PxgU0UGr2-7q"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local[*]').getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_rUnkF247N9m"
      },
      "outputs": [],
      "source": [
        "df = spark.read.csv('/content/boston.csv',\n",
        "                    inferSchema=True,header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtD0zmrS7SRP",
        "outputId": "1ca0c047-6697-4dea-c13e-1f224af657e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+----+\n",
            "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM|  AGE|   DIS|RAD|  TAX|PTRATIO|     B|LSTAT|MEDV|\n",
            "+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+----+\n",
            "|0.00632|18.0| 2.31| 0.0|0.538|6.575| 65.2|  4.09|1.0|296.0|   15.3| 396.9| 4.98|24.0|\n",
            "|0.02731| 0.0| 7.07| 0.0|0.469|6.421| 78.9|4.9671|2.0|242.0|   17.8| 396.9| 9.14|21.6|\n",
            "|0.02729| 0.0| 7.07| 0.0|0.469|7.185| 61.1|4.9671|2.0|242.0|   17.8|392.83| 4.03|34.7|\n",
            "|0.03237| 0.0| 2.18| 0.0|0.458|6.998| 45.8|6.0622|3.0|222.0|   18.7|394.63| 2.94|33.4|\n",
            "|0.06905| 0.0| 2.18| 0.0|0.458|7.147| 54.2|6.0622|3.0|222.0|   18.7| 396.9| 5.33|36.2|\n",
            "|0.02985| 0.0| 2.18| 0.0|0.458| 6.43| 58.7|6.0622|3.0|222.0|   18.7|394.12| 5.21|28.7|\n",
            "|0.08829|12.5| 7.87| 0.0|0.524|6.012| 66.6|5.5605|5.0|311.0|   15.2| 395.6|12.43|22.9|\n",
            "|0.14455|12.5| 7.87| 0.0|0.524|6.172| 96.1|5.9505|5.0|311.0|   15.2| 396.9|19.15|27.1|\n",
            "|0.21124|12.5| 7.87| 0.0|0.524|5.631|100.0|6.0821|5.0|311.0|   15.2|386.63|29.93|16.5|\n",
            "|0.17004|12.5| 7.87| 0.0|0.524|6.004| 85.9|6.5921|5.0|311.0|   15.2|386.71| 17.1|18.9|\n",
            "|0.22489|12.5| 7.87| 0.0|0.524|6.377| 94.3|6.3467|5.0|311.0|   15.2|392.52|20.45|15.0|\n",
            "|0.11747|12.5| 7.87| 0.0|0.524|6.009| 82.9|6.2267|5.0|311.0|   15.2| 396.9|13.27|18.9|\n",
            "|0.09378|12.5| 7.87| 0.0|0.524|5.889| 39.0|5.4509|5.0|311.0|   15.2| 390.5|15.71|21.7|\n",
            "|0.62976| 0.0| 8.14| 0.0|0.538|5.949| 61.8|4.7075|4.0|307.0|   21.0| 396.9| 8.26|20.4|\n",
            "|0.63796| 0.0| 8.14| 0.0|0.538|6.096| 84.5|4.4619|4.0|307.0|   21.0|380.02|10.26|18.2|\n",
            "|0.62739| 0.0| 8.14| 0.0|0.538|5.834| 56.5|4.4986|4.0|307.0|   21.0|395.62| 8.47|19.9|\n",
            "|1.05393| 0.0| 8.14| 0.0|0.538|5.935| 29.3|4.4986|4.0|307.0|   21.0|386.85| 6.58|23.1|\n",
            "| 0.7842| 0.0| 8.14| 0.0|0.538| 5.99| 81.7|4.2579|4.0|307.0|   21.0|386.75|14.67|17.5|\n",
            "|0.80271| 0.0| 8.14| 0.0|0.538|5.456| 36.6|3.7965|4.0|307.0|   21.0|288.99|11.69|20.2|\n",
            "| 0.7258| 0.0| 8.14| 0.0|0.538|5.727| 69.5|3.7965|4.0|307.0|   21.0|390.95|11.28|18.2|\n",
            "+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "JccYCf00StuF"
      },
      "outputs": [],
      "source": [
        "df_study = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V1_aSpXRd92",
        "outputId": "0563da57-f781-41a3-e1c2-5a0c06d683e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CRIM: double (nullable = true)\n",
            " |-- ZN: double (nullable = true)\n",
            " |-- INDUS: double (nullable = true)\n",
            " |-- CHAS: double (nullable = true)\n",
            " |-- NOX: double (nullable = true)\n",
            " |-- RM: double (nullable = true)\n",
            " |-- AGE: double (nullable = true)\n",
            " |-- DIS: double (nullable = true)\n",
            " |-- RAD: double (nullable = true)\n",
            " |-- TAX: double (nullable = true)\n",
            " |-- PTRATIO: double (nullable = true)\n",
            " |-- B: double (nullable = true)\n",
            " |-- LSTAT: double (nullable = true)\n",
            " |-- MEDV: double (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "input_features = [\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\",\"MEDV\"]\n",
        "df_assembler = VectorAssembler(inputCols=input_features,outputCol='features')\n",
        "df_cover_type = df_assembler.transform(df_study)\n",
        "df_cover_type.printSchema()\n",
        "df_train,df_test = df_cover_type.randomSplit([0.75,0.25])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "3EGO2QzrjuTd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05fe62da-910a-45e9-fd6c-0547a27fda13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average RMSE is: 0.3620950275720272\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "sum = 0\n",
        "calculateNum = 100\n",
        "\n",
        "for i in range(calculateNum):\n",
        "  lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8).setLabelCol(\"MEDV\")\n",
        "  lrModel = lr.fit(df_train)\n",
        "  lrPredict = lrModel.transform(df_test)\n",
        "  trainingSummary = lrModel.summary\n",
        "  sum += trainingSummary.rootMeanSquaredError\n",
        "\n",
        "avrgRMSE = sum/calculateNum\n",
        "print(\"Average RMSE is:\", avrgRMSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "hL4B2wxVy3U4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "801b2e59-f8e6-4cb6-c3d9-b4d694687982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average RMSE is: 0.3699083081902787\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.regression import GeneralizedLinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "sum = 0\n",
        "\n",
        "for i in range(calculateNum):\n",
        "  glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3).setLabelCol(\"MEDV\")\n",
        "  glrModel = glr.fit(df_train)\n",
        "  predict = glrModel.transform(df_test)\n",
        "  evaluator = RegressionEvaluator(\n",
        "  labelCol=\"MEDV\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "  rmse = evaluator.evaluate(predict)\n",
        "  sum += rmse\n",
        "\n",
        "avrgRMSE = sum/calculateNum\n",
        "print(\"Average RMSE is:\", avrgRMSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "v3xSv5hF1CE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d18bd9e-295f-42f2-a708-cb63251802af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average RMSE is: 0.9766597056395634\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "\n",
        "sum = 0\n",
        "\n",
        "for i in range(calculateNum):\n",
        "  inputData = df\n",
        "  assembler = VectorAssembler(\n",
        "  inputCols=[\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\",\"MEDV\"],\n",
        "  outputCol=\"features\")\n",
        "  output = assembler.transform(inputData)\n",
        "\n",
        "  dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol='MEDV')\n",
        "  dtModel = dt.fit(df_train)\n",
        "  predictions = dtModel.transform(df_test)\n",
        "  evaluator = RegressionEvaluator(\n",
        "  labelCol=\"MEDV\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "  rmse = evaluator.evaluate(predictions)\n",
        "  sum += rmse\n",
        "\n",
        "avrgRMSE = sum/calculateNum\n",
        "print(\"Average RMSE is:\", avrgRMSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "wg5ws_rz-AY8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e5ccb3-2096-49f3-90a0-9921fd3c5616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average RMSE is: 1.5862399368628237\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "\n",
        "sum = 0\n",
        "\n",
        "for i in range(calculateNum):\n",
        "  inputData = df\n",
        "  assembler = VectorAssembler(\n",
        "  inputCols=[\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\"],\n",
        "  outputCol=\"features\")\n",
        "  output = assembler.transform(inputData)\n",
        "\n",
        "  rf = RandomForestRegressor(featuresCol=\"features\", labelCol='MEDV', numTrees=10)\n",
        "  rfModel = rf.fit(df_train)\n",
        "  predictions = rfModel.transform(df_test)\n",
        "  evaluator = RegressionEvaluator(\n",
        "  labelCol=\"MEDV\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "  rmse = evaluator.evaluate(predictions)\n",
        "  sum += rmse\n",
        "\n",
        "avrgRMSE = sum/calculateNum\n",
        "print(\"Average RMSE is:\", avrgRMSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "n74KholORNVy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02c2c154-b841-4781-efb0-7a0fdd2023e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average RMSE is: 1.0064015592174191\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "\n",
        "sum = 0\n",
        "\n",
        "for i in range(calculateNum):\n",
        "  inputData = df\n",
        "  assembler = VectorAssembler(\n",
        "  inputCols=[\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\"],\n",
        "  outputCol=\"features\")\n",
        "  output = assembler.transform(inputData)\n",
        "\n",
        "  gbt = GBTRegressor(featuresCol=\"features\", labelCol='MEDV')\n",
        "  gbtModel = gbt.fit(df_train)\n",
        "  predictions = gbtModel.transform(df_test)\n",
        "  evaluator = RegressionEvaluator(\n",
        "  labelCol=\"MEDV\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "  rmse = evaluator.evaluate(predictions)\n",
        "  sum += rmse\n",
        "\n",
        "avrgRMSE = sum/calculateNum\n",
        "print(\"Average RMSE is:\", avrgRMSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "do14RnrbdPfI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "68039551-bda5-48e8-daa5-4e5124ba70d8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-33b573479739>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mafts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAFTSurvivalRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturesCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'MEDV'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0maftsModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mafts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maftsModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   evaluator = RegressionEvaluator(\n",
            "\u001b[0;32m/content/spark-3.3.0-bin-hadoop3/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/content/spark-3.3.0-bin-hadoop3/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.0-bin-hadoop3/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.0-bin-hadoop3/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.3.0-bin-hadoop3/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: censor does not exist. Available: CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, B, LSTAT, MEDV, features"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.regression import AFTSurvivalRegression\n",
        "\n",
        "sum = 0\n",
        "\n",
        "for i in range(calculateNum):\n",
        "  inputData = df\n",
        "  assembler = VectorAssembler(\n",
        "  inputCols=[\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\"],\n",
        "  outputCol=\"features\")\n",
        "  output = assembler.transform(inputData)\n",
        "\n",
        "  afts = AFTSurvivalRegression(featuresCol=\"features\", labelCol='MEDV')\n",
        "  aftsModel = afts.fit(df_train)\n",
        "  predictions = aftsModel.transform(df_test)\n",
        "  evaluator = RegressionEvaluator(\n",
        "  labelCol=\"MEDV\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "  rmse = evaluator.evaluate(predictions)\n",
        "  sum += rmse\n",
        "\n",
        "avrgRMSE = sum/calculateNum\n",
        "print(\"Average RMSE is:\", avrgRMSE)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}